{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- The os module has a perfect method to list files in a directory.\n",
    "- Pandas json normalize could work here but is not necessary to convert the JSON data to a dataframe.\n",
    "- You may need a nested for-loop to access each sale!\n",
    "- We've put a lot of time into creating the structure of this repository, and it's a good example for future projects.  In the file functions_variables.py, there is an example function that you can import and use.  If you have any variables, functions or classes that you want to make, they can be put in the functions_variables.py file and imported into a notebook.  Note that only .py files can be imported into a notebook. If you want to import everything from a .py file, you can use the following:\n",
    "```python\n",
    "from functions_variables import *\n",
    "```\n",
    "If you just import functions_variables, then each object from the file will need to be prepended with \"functions_variables\"\\\n",
    "Using this .py file will keep your notebooks very organized and make it easier to reuse code between notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (this is not an exhaustive list of libraries)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from pprint import pprint\n",
    "from functions_variables import encode_tags\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from category_encoders import TargetEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load one file first to see what type of data you're dealing with and what attributes it has\n",
    "\n",
    "# Define the data directory and file path\n",
    "current_dir = os.getcwd()\n",
    "main_dir = os.path.abspath(os.path.join(current_dir, '..'))\n",
    "data_dir = os.path.join(main_dir, 'data')\n",
    "sample_file = os.path.join(data_dir, \"AK_Juneau_0.json\")\n",
    "\n",
    "# Load JSON data\n",
    "with open(sample_file, \"r\") as f:\n",
    "    sample_data = json.load(f)\n",
    "\n",
    "# Convert JSON to a DataFrame\n",
    "json_structure = pd.DataFrame(sample_data)\n",
    "\n",
    "# Creating a file for csv data:\n",
    "csv_dir = os.path.join(main_dir, 'csv_data')\n",
    "os.makedirs(csv_dir, exist_ok=True)\n",
    "\n",
    "# Saving json_structure df to csv: \n",
    "json_structure.to_csv(os.path.join(csv_dir, 'json_structure.csv'), index=False)\n",
    "print(\"CSV file created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print a preview of the data\n",
    "print(json.dumps(sample_data, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over all files and put them into a dataframe\n",
    "\n",
    "# Initialize a list to store extracted sale records\n",
    "data_list = []\n",
    "\n",
    "# Loop through each JSON file\n",
    "for file in os.listdir(data_dir):\n",
    "    if not file.endswith('.json'):\n",
    "        continue\n",
    "        \n",
    "    file_path = os.path.join(data_dir, file)  # Construct full file path\n",
    "    \n",
    "    with open(file_path, \"r\") as f:\n",
    "        raw_data = json.load(f)  # Load JSON data\n",
    "        \n",
    "        # Extract property listings (assuming structure is in \"data\" -> \"results\")\n",
    "        listings = raw_data.get(\"data\", {}).get(\"results\", [])\n",
    "\n",
    "        if not isinstance(listings, list):  # Check if \"results\" is not a list\n",
    "            print(f\"Skipping malformed file: {file}\")\n",
    "            continue  # Skip files without proper listings\n",
    "\n",
    "        # Process each listing\n",
    "        for listing in listings:\n",
    "            sale_record = {\n",
    "                \"property_id\": listing.get(\"property_id\", \"Unknown\"),\n",
    "                \"permalink\": listing.get(\"permalink\", \"Unknown\"),\n",
    "                \"status\": listing.get(\"status\", \"Unknown\"),\n",
    "                \"year_built\": listing.get(\"description\", {}).get(\"year_built\", None),\n",
    "                \"garage\": listing.get(\"description\", {}).get(\"garage\", None),\n",
    "                \"stories\": listing.get(\"description\", {}).get(\"stories\", None),\n",
    "                \"beds\": listing.get(\"description\", {}).get(\"beds\", None),\n",
    "                \"baths_1qtr\": listing.get(\"description\", {}).get(\"baths_1qtr\", None),\n",
    "                \"baths_3qtr\": listing.get(\"description\", {}).get(\"baths_3qtr\", None),\n",
    "                \"baths_half\": listing.get(\"description\", {}).get(\"baths_half\", None),\n",
    "                \"baths_full\": listing.get(\"description\", {}).get(\"baths_full\", None),\n",
    "                \"baths\": listing.get(\"description\", {}).get(\"baths\", \"Unknown\"),\n",
    "                \"type\": listing.get(\"description\", {}).get(\"type\", \"Unknown\"),\n",
    "                \"sub_type\": listing.get(\"description\", {}).get(\"sub_type\", \"Unknown\"),\n",
    "                \"lot_sqft\": listing.get(\"description\", {}).get(\"lot_sqft\", None),\n",
    "                \"sqft\": listing.get(\"description\", {}).get(\"sqft\", None),\n",
    "                \"sold_price\": listing.get(\"description\", {}).get(\"sold_price\", None),\n",
    "                \"sold_date\": pd.to_datetime(listing.get(\"description\", {}).get(\"sold_date\", \"Unknown\")),\n",
    "                \"list_price\": listing.get(\"list_price\", None),\n",
    "                \"last_update_date\": pd.to_datetime(listing.get(\"last_update_date\"), errors='coerce'),\n",
    "                \"city\": listing.get(\"location\", {}).get(\"address\", {}).get(\"city\", \"Unknown\"),\n",
    "                \"state\": listing.get(\"location\", {}).get(\"address\", {}).get(\"state\", \"Unknown\"),\n",
    "                \"postal_code\": listing.get(\"location\", {}).get(\"address\", {}).get(\"postal_code\", \"Unknown\"),\n",
    "                \"street_view_url\": listing.get(\"location\", {}).get(\"street_view_url\", \"Unknown\"),\n",
    "                \"tags\": listing.get(\"tags\", [])  \n",
    "            }\n",
    "            data_list.append(sale_record)\n",
    "\n",
    "\n",
    "# Convert extracted data into a DataFrame\n",
    "listings_df = pd.DataFrame(data_list)\n",
    "\n",
    "# Save to CSV for easier analysis\n",
    "listings_df.to_csv(os.path.join(csv_dir, \"raw_real_estate_listings.csv\"), index=False)\n",
    "print(\"CSV file created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning and Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, ensure that you have all sales in a dataframe.\n",
    "- Take a quick look at your data (i.e. `.info()`, `.describe()`) - what do you see?\n",
    "- Is each cell one value, or do some cells have lists?\n",
    "- What are the data types of each column?\n",
    "- Some sales may not actually include the sale price (target).  These rows should be dropped.\n",
    "- There are a lot of NA/None values.  Should these be dropped or replaced with something?\n",
    "    - You can drop rows or use various methods to fills NA's - use your best judgement for each column \n",
    "    - i.e. for some columns (like Garage), NA probably just means no Garage, so 0\n",
    "- Drop columns that aren't needed\n",
    "    - Don't keep the list price because it will be too close to the sale price. Assume we want to predict the price of houses not yet listed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listings_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first glance, `baths_1qtr` has all null values and should be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listings_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading and inspecting the data\n",
    "listings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping entrites with duplicate 'property_id'\n",
    "\n",
    "listings_df = listings_df.drop_duplicates(subset='property_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modifying datatypes\n",
    "\n",
    "# float to int\n",
    "listings_df.loc[:, 'year_built'] = listings_df['year_built'].astype('Int64') \n",
    "listings_df.loc[:, 'garage'] = listings_df['garage'].astype('Int64')\n",
    "listings_df.loc[:, 'stories'] = listings_df['stories'].astype('Int64')\n",
    "listings_df.loc[:, 'beds'] = listings_df['beds'].astype('Int64')\n",
    "listings_df.loc[:, 'baths'] = listings_df['baths'].astype('Int64')\n",
    "\n",
    "# Remove time from last_update_date and convert to datetime64\n",
    "listings_df.loc[:, 'last_update_date'] = pd.to_datetime(listings_df['last_update_date'].astype('str').str[:10], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Addressing Null values \n",
    "\n",
    "pd.DataFrame(listings_df.isnull().sum() * 100 / len(listings_df)).reset_index().sort_values(0, ascending=False).rename(columns={'index': 'column_name',\n",
    "                                                                                                              0: 'percent_missing'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the 4 columns wehere over 50% of the data is null (baths_1qtr)\n",
    "listings_df = listings_df.drop(columns=['baths_1qtr','baths_3qtr', 'sub_type', 'baths_half'])\n",
    "\n",
    "# Dropping the redundant 'baths_full' column in favor of the 'baths' column\n",
    "listings_df = listings_df.drop(columns='baths_full')\n",
    "\n",
    "# Dropping the 'list_price' column in favor of 'sold_price' to predict the prices of houses not listed\n",
    "listings_df = listings_df.drop(columns='list_price')\n",
    "\n",
    "# Dropping the rows that have a null sale price (target)\n",
    "listings_df = listings_df[~listings_df['sold_price'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To suppress the warning that results from filling last_update_date \n",
    "pd.set_option('future.no_silent_downcasting', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Null value replacement\n",
    "\n",
    "# if none then 0: \n",
    "listings_df['garage'] = listings_df['garage'].fillna(value=0) \n",
    "listings_df['lot_sqft'] = listings_df['lot_sqft'].fillna(value=0)\n",
    "listings_df['sqft'] = listings_df['sqft'].fillna(value=0)\n",
    "listings_df['beds'] = listings_df['beds'].fillna(value=0)\n",
    "listings_df['baths'] = listings_df['baths'].fillna(value=0)\n",
    "listings_df['tags'] = listings_df['tags'].fillna(value='[]')\n",
    "\n",
    "listings_df['last_update_date'] = listings_df['last_update_date'].fillna(value=listings_df['sold_date']) # Assuming the listing was last updated on the day it sold \n",
    "print('null type count:', listings_df[listings_df['type'].isnull() ==True].shape[0])\n",
    "listings_df['type'] = listings_df['type'].fillna(value='single_family') # Only a few entries, validated type manually through zillow search \n",
    "\n",
    "print('null city count:', listings_df[listings_df['city'].isnull() ==True].shape[0])\n",
    "listings_df['city'] = listings_df['city'].fillna(value='Columbus') # Only one entry, validated city manually through search \n",
    "\n",
    "listings_df['stories'] = listings_df['stories'].fillna(value=0) # Manually validated some addresses with stories null, appears to be warehouses, fields, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Null year_built:\n",
    "\n",
    "null_years_built = listings_df[listings_df['year_built'].isnull() == True]\n",
    "null_years_built"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_years_built.groupby('type').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listings_df.groupby(by=['type', 'city'])['year_built'].min()['land']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listings_df.groupby(by=['type', 'city'])['year_built'].min()['condo']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The majority of records that have null `year_built` are of `land` or `single_family` type. For those of `single_family` type, values will be grouped by type and city, and the most frequently occured year will be the replacement. Only one record of 'year_built' exists for the listings of land type, the year 1956 in Phoenix, Arizona. This value is used to impute records with null 'year_built' that is of land type. It is important to note that this imputation will reduce the variability of the data. A similar issue exists for those of the condo type, with the only record of the year 1985 in Nashville. The same imputation method will be used. The two entries that are of type 'other' are the only entries in the dataframe of that type, and upon manual inspection the listings are for the lot or incomplete so they will be dropped. After the conditions are applied, there are only 3 records left, whose year_built information cannot be found online. To maintain more data integretity, these records will be dropped instead of trying to find a statristic to genneralize. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listings_df[listings_df['year_built'].isnull() == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace null values for 'year_built'\n",
    "\n",
    "# Define conditions \n",
    "condition_sf = (listings_df['type'] == 'single_family') & (listings_df['year_built'].isna())\n",
    "condition_land = (listings_df['type'] == 'land') & (listings_df['year_built'].isna())\n",
    "condition_condo = (listings_df['type'] == 'condo') & (listings_df['year_built'].isna())\n",
    "condition_mobile = (listings_df['type'] == 'mobile') & (listings_df['year_built'].isna())\n",
    "# Manual search conditions\n",
    "condition_ms1 = (listings_df['property_id'] == '5328654220') & (listings_df['year_built'].isna())   \n",
    "condition_ms2 = (listings_df['property_id'] == '9414271198') & (listings_df['year_built'].isna())\n",
    "\n",
    "condition_ms3 = (listings_df['property_id'] == '9988039199') & (listings_df['year_built'].isna())\n",
    "condition_ms4 = (listings_df['property_id'] == '3377433199') & (listings_df['year_built'].isna())\n",
    "\n",
    "# Group by type and city, then get mode of rows of single family type\n",
    "sf_modes = (listings_df[listings_df['type'] == 'single_family'].groupby(['type', 'city'])['year_built'].agg(lambda x: x.mode().iloc[0] if not x.mode().empty else pd.NA))\n",
    "\n",
    "# Define a function to get the mode from the groupby\n",
    "def get_mode(row):\n",
    "    try:\n",
    "        return sf_modes.loc[(row['type'], row['city'])]\n",
    "    except KeyError:\n",
    "        return row['year_built']  # fallback if no mode available\n",
    "\n",
    "# Apply the function to the single_family condition\n",
    "listings_df.loc[condition_sf, 'year_built'] = listings_df[condition_sf].apply(get_mode, axis=1)\n",
    "print('null type count after single_family modes:', listings_df[listings_df['year_built'].isnull() ==True].shape[0])\n",
    "\n",
    "# Manually validating and imputation everything else\n",
    "listings_df.loc[condition_land, 'year_built'] = 1956\n",
    "listings_df.loc[condition_condo, 'year_built'] = 1985\n",
    "listings_df = listings_df[listings_df['type'] != 'other']\n",
    "listings_df.loc[condition_mobile, 'year_built'] = 1988\n",
    "\n",
    "# Values obtained through manual search\n",
    "listings_df.loc[condition_ms1, 'year_built'] = 1876\n",
    "listings_df.loc[condition_ms2, 'year_built'] = 2023\n",
    "listings_df.loc[condition_ms3, 'year_built'] = 1983\n",
    "listings_df.loc[condition_ms4, 'year_built'] = 1981\n",
    "print('null type count after manual conditions:', listings_df[listings_df['year_built'].isnull() ==True].shape[0])\n",
    "\n",
    "# Dropping remaining rows\n",
    "listings_df = listings_df[listings_df['year_built'].isnull() == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop remaining redundant columns \n",
    "listings_df = listings_df.drop(columns=['permalink', 'postal_code', 'street_view_url'])\n",
    "\n",
    "# Checking that all listings are confirmed to be sold before dropping the status column\n",
    "print('Listing statuses:')\n",
    "print(listings_df['status'].unique())\n",
    "listings_df = listings_df.drop(columns='status')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate cleaned dataframe and save to csv\n",
    "display(listings_df.head())\n",
    "display(listings_df.info())\n",
    "\n",
    "listings_df.to_csv(os.path.join(csv_dir, \"cleaned_real_estate_listings.csv\"), index=False)\n",
    "print(\"CSV file created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of columns: {listings_df.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering\n",
    "\n",
    "listings_df['total_sqft'] = sum(listings_df['lot_sqft'], listings_df['sqft'])\n",
    "listings_df['building_ratio'] = listings_df['sqft'] / listings_df['lot_sqft'] # higher ratio = more building area(urban), 0 = all lot no building\n",
    "listings_df['building_ratio'] = listings_df['building_ratio'].replace([np.inf, -np.inf], np.nan) # fixing cases where ratio is inf (0 sqft and positive lot sqft) \n",
    "listings_df['building_ratio'] = listings_df['building_ratio'].fillna(value=listings_df['building_ratio'].mean()) # filling nulls with global building ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with Tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the fact that with tags, there are a lot of categorical variables.\n",
    "- How many columns would we have if we OHE tags, city and state?\n",
    "- Perhaps we can get rid of tags that have a low frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OHE categorical variables/ tags here\n",
    "# tags will have to be done manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate the Number of Columns Created by OHE\n",
    "print(f\"Unique tags: {listings_df['tags'].explode().nunique()}\")\n",
    "print(f\"Unique cities: {listings_df['city'].nunique()}\")\n",
    "print(f\"Unique states: {listings_df['state'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode tags\n",
    "listings_df = encode_tags(listings_df, min_occurrences=100)\n",
    "\n",
    "# Dropping original tag column \n",
    "listings_df = listings_df.drop(columns='tags')\n",
    "\n",
    "# Check updated column count\n",
    "print(f\"Updated column count: {len(listings_df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with Cities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sales will vary drastically between cities and states.  Is there a way to keep information about which city it is without OHE?\n",
    "- Could we label encode or ordinal encode?  Yes, but this may have undesirable effects, giving nominal data ordinal values.\n",
    "- What we can do is use our training data to encode the mean sale price by city as a feature (a.k.a. Target Encoding)\n",
    "    - We can do this as long as we ONLY use the training data - we're using the available data to give us a 'starting guess' of the price for each city, without needing to encode city explicitly\n",
    "- If you replace cities or states with numerical values (like the mean price), make sure that the data is split so that we don't leak data into the training selection. This is a great time to train test split. Compute on the training data, and join these values to the test data\n",
    "- Note that you *may* have cities in the test set that are not in the training set. You don't want these to be NA, so maybe you can fill them with the overall mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform train test split here\n",
    "from category_encoders import TargetEncoder\n",
    "\n",
    "# Using k-fold CV with shuffle for a dataset this size \n",
    "X = listings_df.drop(columns='sold_price')\n",
    "y = listings_df['sold_price']\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=13)\n",
    "\n",
    "# Isolating train/test dataframes from k-fold split\n",
    "for train_idx, test_idx in kf.split(X):\n",
    "    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "    # Initiating target encoder and only transforming the training set     \n",
    "    encoder = TargetEncoder(cols=['city', 'state'], smoothing=3)\n",
    "    encoded_data = encoder.fit_transform(X_train[['city', 'state']], y_train)\n",
    "\n",
    "    # Inserting encoded data into main dataframe\n",
    "    listings_df['encoded_city'] = round((encoded_data['city']), 2) \n",
    "    listings_df['encoded_state'] = round((encoded_data['state']), 2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viewing encoded city and state columns side by side to validate\n",
    "\n",
    "city_encodings = listings_df[['city', 'encoded_city']].dropna(subset=['encoded_city']).drop_duplicates('city').sort_values(by='encoded_city', ascending=False)\n",
    "state_encodings = listings_df[['state', 'encoded_state']].dropna(subset=['encoded_state']).drop_duplicates('state').sort_values(by='encoded_state', ascending=False)\n",
    "\n",
    "display(city_encodings.head(10))\n",
    "display(state_encodings.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use encoded values to fill in test set \n",
    "city_mappings = city_encodings.set_index('city')['encoded_city']\n",
    "state_mappings = state_encodings.set_index('state')['encoded_state']\n",
    "\n",
    "listings_df['encoded_city'] = listings_df['encoded_city'].fillna(listings_df['city'].map(city_mappings))\n",
    "listings_df['encoded_state'] = listings_df['encoded_state'].fillna(listings_df['state'].map(state_mappings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for nulls, replace with global city/state means if exist \n",
    "\n",
    "overall_mean_price = listings_df[\"sold_price\"].mean()\n",
    "listings_df[\"encoded_city\"] = listings_df[\"encoded_city\"].fillna(overall_mean_price)\n",
    "listings_df[\"encoded_state\"] = listings_df[\"encoded_state\"].fillna(overall_mean_price)\n",
    "listings_df[\"encoded_city\"] = listings_df[\"encoded_city\"].astype('float')\n",
    "listings_df[\"encoded_state\"] = listings_df[\"encoded_state\"].astype('float')\n",
    "\n",
    "listings_df[['encoded_city', 'encoded_state']].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding remaining categorical columns \n",
    "\n",
    "# Conditions for encoding type\n",
    "type1 = listings_df['type'] == 'mobile'\n",
    "type2 = listings_df['type'] == 'apartment' \n",
    "type3 = listings_df['type'] == 'condo'\n",
    "type4 = listings_df['type'] == 'condos'\n",
    "type5 = listings_df['type'] == 'condo_townhome_rowhome_coop'\n",
    "type6 = listings_df['type'] == 'townhomes'\n",
    "type7 = listings_df['type'] == 'single_family'\n",
    "type8 = listings_df['type'] == 'duplex_triplex'\n",
    "type9 = listings_df['type'] == 'multi_family'\n",
    "type10 = listings_df['type'] == 'land'\n",
    "\n",
    "listings_df.loc[type1, 'type'] = 1.0\n",
    "listings_df.loc[type2, 'type'] = 2\n",
    "listings_df.loc[type3, 'type'] = 3\n",
    "listings_df.loc[type4, 'type'] = 4\n",
    "listings_df.loc[type5, 'type'] = 5\n",
    "listings_df.loc[type6, 'type'] = 6\n",
    "listings_df.loc[type7, 'type'] = 7\n",
    "listings_df.loc[type8, 'type'] = 8\n",
    "listings_df.loc[type9, 'type'] = 9\n",
    "listings_df.loc[type10, 'type'] = 10\n",
    "listings_df['type'] = listings_df['type'].astype('Int64')      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listings_df.select_dtypes(include=object) # drop property id and last update date? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many listings of the 'mobile' type have 0 sqft in total after feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save encoded dataset \n",
    "listings_df.to_csv(os.path.join(csv_dir, \"encoded_real_estate_listings.csv\"), index=False)\n",
    "\n",
    "print(\"Encoded dataset saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Data - STRETCH\n",
    "\n",
    "> This doesn't need to be part of your Minimum Viable Product (MVP). We recommend you write a functional, basic pipeline first, then circle back and join new data if you have time\n",
    "\n",
    "> If you do this, try to write your downstream steps in a way it will still work on a dataframe with different features!\n",
    "\n",
    "- You're not limited to just using the data provided to you. Think/ do some research about other features that might be useful to predict housing prices. \n",
    "- Can you import and join this data? Make sure you do any necessary preprocessing and make sure it is joined correctly.\n",
    "- Example suggestion: could mortgage interest rates in the year of the listing affect the price? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import, join and preprocess new data here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA/ Visualization\n",
    "\n",
    "Remember all of the EDA that you've been learning about?  Now is a perfect time for it!\n",
    "- Look at distributions of numerical variables to see the shape of the data and detect outliers.    \n",
    "    - Consider transforming very skewed variables\n",
    "- Scatterplots of a numerical variable and the target go a long way to show correlations.\n",
    "- A heatmap will help detect highly correlated features, and we don't want these.\n",
    "    - You may have too many features to do this, in which case you can simply compute the most correlated feature-pairs and list them\n",
    "- Is there any overlap in any of the features? (redundant information, like number of this or that room...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = listings_df[['type', 'city', 'state']]\n",
    "num_cols = listings_df[['year_built', 'garage', 'stories', 'beds', 'baths', 'lot_sqft', 'sqft', 'sold_price', 'total_sqft', 'building_ratio']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "box_cols = listings_df[['garage', 'stories', 'beds', 'baths']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in num_cols:\n",
    "    sns.displot(listings_df[column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in box_cols: \n",
    "    sns.boxplot(listings_df[[column]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Histogram to check distributions\n",
    "test_df.hist(figsize=(20, 8), bins=30)\n",
    "plt.show()\n",
    "\n",
    "# Boxplot to detect outliers\n",
    "sns.boxplot(data=listings_df[['year_built', 'beds', 'baths']])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_avg_price = listings_df.groupby(\"state\")[\"sold_price\"].mean().reset_index()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=\"sold_price\", y=\"state\", data=state_avg_price)\n",
    "plt.xticks(rotation=90)\n",
    "plt.title(\"Average Sale Price by State\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=listings_df[\"year_built\"], y=listings_df[\"sqft\"])\n",
    "plt.xlabel(\"Year Built\")\n",
    "plt.ylabel(\"square feet\")\n",
    "plt.title(\"Square Feet vs. Year built\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=listings_df[\"sqft\"], y=listings_df[\"sold_price\"])\n",
    "plt.xlabel(\"Square Feet\")\n",
    "plt.ylabel(\"Sale Price\")\n",
    "plt.title(\"Square Feet vs. Sale Price\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=listings_df[\"total_sqft\"], y=listings_df[\"sold_price\"])\n",
    "plt.xlabel(\"Square Feet\")\n",
    "plt.ylabel(\"Sale Price\")\n",
    "plt.title(\"Square Feet vs. Sale Price\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.regplot(x=listings_df[\"sqft\"], y=listings_df[\"sold_price\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.regplot(x=listings_df[\"encoded_state\"], y=listings_df[\"sold_price\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(listings_df[['year_built', 'garage', 'stories', 'beds', 'baths', 'lot_sqft', 'sqft', 'sold_price', 'encoded_city', 'encoded_state', 'total_sqft', 'building_ratio']].corr(numeric_only=True), annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling and Finishing Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now is a great time to scale the data and save it once it's preprocessed.\n",
    "- You can save it in your data folder, but you may want to make a new `processed/` subfolder to keep it organized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "# Choose scaling method\n",
    "scaler = StandardScaler()  # Use MinMaxScaler() if needed\n",
    "\n",
    "# Select numeric columns for scaling\n",
    "cols_to_scale = ['lot_sqft', 'sqft', 'total_sqft', 'building_ratio', 'encoded_city', 'encoded_state']\n",
    "listings_df[cols_to_scale] = scaler.fit_transform(listings_df[cols_to_scale])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file path for processed data \n",
    "processed_dir = os.path.join(main_dir, 'processed_data')\n",
    "os.makedirs(processed_dir, exist_ok=True)\n",
    "\n",
    "# Save the processed dataset\n",
    "processed_listings = listings_df.drop(columns=['city', 'state'])\n",
    "processed_listings.to_csv(os.path.join(processed_dir, 'processed_real_estate_listings.csv'), index=False)\n",
    "print(\"Processed dataset saved successfully!\")\n",
    "\n",
    "# Train/test split on processed dataset and save to csv\n",
    "X = processed_listings.drop(columns='sold_price')\n",
    "y = processed_listings['sold_price']\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=13)\n",
    "\n",
    "for train_idx, test_idx in kf.split(X):\n",
    "    train_df = X.iloc[train_idx].copy()\n",
    "    test_df = X.iloc[test_idx].copy()\n",
    "\n",
    "train_df.to_csv(os.path.join(processed_dir, 'train_df.csv'), index=False)\n",
    "test_df.to_csv(os.path.join(processed_dir, 'test_df.csv'), index=False)\n",
    "print(\"Train/test datasets saved successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lhl_env",
   "language": "python",
   "name": "lhl_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
