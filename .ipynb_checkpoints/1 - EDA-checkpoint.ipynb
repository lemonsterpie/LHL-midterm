{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The os module has a perfect method to list files in a directory.\n",
    "Pandas json normalize could work here but is not necessary to convert the JSON data to a dataframe.\n",
    "You may need a nested for-loop to access each sale!\n",
    "We've put a lot of time into creating the structure of this repository, and it's a good example for future projects. In the file functions_variables.py, there is an example function that you can import and use. If you have any variables, functions or classes that you want to make, they can be put in the functions_variables.py file and imported into a notebook. Note that only .py files can be imported into a notebook. If you want to import everything from a .py file, you can use the following:\n",
    "from functions_variables import *\n",
    "If you just import functions_variables, then each object from the file will need to be prepended with \"functions_variables\"\n",
    "Using this .py file will keep your notebooks very organized and make it easier to reuse code between notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (this is not an exhaustive list of libraries)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from pprint import pprint\n",
    "from functions_variables import encode_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file created successfully!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the data directory and file path\n",
    "data_dir = \"/Users/erum/LHL-midterm/data\"\n",
    "sample_file = os.path.join(data_dir, \"AK_Juneau_0.json\")\n",
    "\n",
    "# Load JSON data\n",
    "with open(sample_file, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Convert JSON to a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save to CSV only if df is defined\n",
    "if not df.empty:\n",
    "    df.to_csv(\"real_estate_listings_augusta.csv\", index=False)\n",
    "    print(\"CSV file created successfully!\")\n",
    "else:\n",
    "    print(\"Warning: The DataFrame is empty. Check the JSON structure.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"status\": 200,\n",
      "    \"data\": {\n",
      "        \"total\": 0,\n",
      "        \"count\": 0,\n",
      "        \"results\": {}\n",
      "    }\n",
      "}\n",
      "CSV file created successfully!\n"
     ]
    }
   ],
   "source": [
    "# load one file first to see what type of data you're dealing with and what attributes it has\n",
    "\n",
    "# Define the data directory\n",
    "data_dir = \"/Users/erum/LHL-midterm/data\"\n",
    "\n",
    "# Inspect one file\n",
    "sample_file = os.path.join(data_dir, \"ME_Augusta_4.json\")\n",
    "\n",
    "with open(sample_file, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Print a preview of the data\n",
    "print(json.dumps(data, indent=4))\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(\"real_estate_listings_augusta.csv\", index=False)\n",
    "print(\"CSV file created successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8159 entries, 0 to 8158\n",
      "Data columns (total 4 columns):\n",
      " #   Column            Non-Null Count  Dtype \n",
      "---  ------            --------------  ----- \n",
      " 0   primary_photo     8159 non-null   object\n",
      " 1   last_update_date  8125 non-null   object\n",
      " 2   source_type       8159 non-null   object\n",
      " 3   agent_offices     8159 non-null   object\n",
      "dtypes: object(4)\n",
      "memory usage: 255.1+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        status  data\n",
      "count      3.0   3.0\n",
      "unique     NaN   2.0\n",
      "top        NaN   8.0\n",
      "freq       NaN   2.0\n",
      "mean     200.0   NaN\n",
      "std        0.0   NaN\n",
      "min      200.0   NaN\n",
      "25%      200.0   NaN\n",
      "50%      200.0   NaN\n",
      "75%      200.0   NaN\n",
      "max      200.0   NaN\n"
     ]
    }
   ],
   "source": [
    "print(df.describe(include=\"all\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping malformed file: ME_Augusta_4.json\n",
      "Skipping malformed file: MS_Jackson_0.json\n",
      "Skipping malformed file: MS_Jackson_1.json\n",
      "Skipping malformed file: WY_Cheyenne_4.json\n",
      "Skipping malformed file: VT_Montpelier_4.json\n",
      "Skipping malformed file: WY_Cheyenne_3.json\n",
      "Skipping malformed file: SD_Pierre_0.json\n",
      "Skipping malformed file: ME_Augusta_2.json\n",
      "Skipping malformed file: VT_Montpelier_3.json\n",
      "Skipping malformed file: ME_Augusta_3.json\n",
      "Skipping malformed file: VT_Montpelier_2.json\n",
      "Skipping malformed file: SD_Pierre_1.json\n",
      "Skipping malformed file: WY_Cheyenne_2.json\n",
      "Skipping malformed file: SD_Pierre_2.json\n",
      "Skipping malformed file: MS_Jackson_4.json\n",
      "Skipping malformed file: NH_Concord_4.json\n",
      "Skipping malformed file: WY_Cheyenne_1.json\n",
      "Skipping malformed file: VT_Montpelier_1.json\n",
      "Skipping malformed file: ME_Augusta_0.json\n",
      "Skipping malformed file: ND_Bismarck_2.json\n",
      "Skipping malformed file: HI_Honolulu_3.json\n",
      "Skipping malformed file: ND_Bismarck_3.json\n",
      "Skipping malformed file: VT_Montpelier_0.json\n",
      "Skipping malformed file: ME_Augusta_1.json\n",
      "Skipping malformed file: WY_Cheyenne_0.json\n",
      "Skipping malformed file: SD_Pierre_3.json\n",
      "Skipping malformed file: ND_Bismarck_4.json\n",
      "Skipping malformed file: SD_Pierre_4.json\n",
      "Skipping malformed file: MS_Jackson_2.json\n",
      "Skipping malformed file: NH_Concord_3.json\n",
      "Skipping malformed file: MS_Jackson_3.json\n",
      "Skipping malformed file: HI_Honolulu_4.json\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8159 entries, 0 to 8158\n",
      "Data columns (total 20 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   property_id       8159 non-null   object \n",
      " 1   permalink         8159 non-null   object \n",
      " 2   status            8159 non-null   object \n",
      " 3   year_built        7316 non-null   float64\n",
      " 4   garage            4448 non-null   float64\n",
      " 5   stories           6260 non-null   float64\n",
      " 6   beds              7504 non-null   float64\n",
      " 7   baths             7980 non-null   float64\n",
      " 8   type              8125 non-null   object \n",
      " 9   lot_sqft          6991 non-null   float64\n",
      " 10  sqft              7323 non-null   float64\n",
      " 11  sold_price        6716 non-null   float64\n",
      " 12  sold_date         8159 non-null   object \n",
      " 13  list_price        7721 non-null   float64\n",
      " 14  last_update_date  8125 non-null   object \n",
      " 15  city              8154 non-null   object \n",
      " 16  state             8159 non-null   object \n",
      " 17  postal_code       8159 non-null   object \n",
      " 18  street_view_url   8159 non-null   object \n",
      " 19  tags              8159 non-null   object \n",
      "dtypes: float64(9), object(11)\n",
      "memory usage: 1.2+ MB\n",
      "None\n",
      "CSV file created successfully!\n"
     ]
    }
   ],
   "source": [
    "# Initialize a list to store extracted sale records\n",
    "data_list = []\n",
    "\n",
    "# Loop through each JSON file\n",
    "for file in json_files:\n",
    "    file_path = os.path.join(data_dir, file)  # Construct full file path\n",
    "    \n",
    "    with open(file_path, \"r\") as f:\n",
    "        raw_data = json.load(f)  # Load JSON data\n",
    "        \n",
    "        # Extract property listings (assuming structure is in \"data\" -> \"results\")\n",
    "        listings = raw_data.get(\"data\", {}).get(\"results\", [])\n",
    "\n",
    "        if not isinstance(listings, list):  # Check if \"results\" is not a list\n",
    "            print(f\"Skipping malformed file: {file}\")\n",
    "            continue  # Skip files without proper listings\n",
    "\n",
    "        # Process each listing\n",
    "        for listing in listings:\n",
    "            sale_record = {\n",
    "                \"property_id\": listing.get(\"property_id\", \"Unknown\"),\n",
    "                \"permalink\": listing.get(\"permalink\", \"Unknown\"),\n",
    "                \"status\": listing.get(\"status\", \"Unknown\"),\n",
    "                \"year_built\": listing.get(\"description\", {}).get(\"year_built\", \"Unknown\"),\n",
    "                \"garage\": listing.get(\"description\", {}).get(\"garage\", \"Unknown\"),\n",
    "                \"stories\": listing.get(\"description\", {}).get(\"stories\", \"Unknown\"),\n",
    "                \"beds\": listing.get(\"description\", {}).get(\"beds\", \"Unknown\"),\n",
    "                \"baths\": listing.get(\"description\", {}).get(\"baths\", \"Unknown\"),\n",
    "                \"type\": listing.get(\"description\", {}).get(\"type\", \"Unknown\"),\n",
    "                \"lot_sqft\": listing.get(\"description\", {}).get(\"lot_sqft\", \"Unknown\"),\n",
    "                \"sqft\": listing.get(\"description\", {}).get(\"sqft\", \"Unknown\"),\n",
    "                \"sold_price\": listing.get(\"description\", {}).get(\"sold_price\", \"Unknown\"),\n",
    "                \"sold_date\": listing.get(\"description\", {}).get(\"sold_date\", \"Unknown\"),\n",
    "                \"list_price\": listing.get(\"list_price\", \"Unknown\"),\n",
    "                \"last_update_date\": listing.get(\"last_update_date\", \"Unknown\"),\n",
    "                \"city\": listing.get(\"location\", {}).get(\"address\", {}).get(\"city\", \"Unknown\"),\n",
    "                \"state\": listing.get(\"location\", {}).get(\"address\", {}).get(\"state\", \"Unknown\"),\n",
    "                \"postal_code\": listing.get(\"location\", {}).get(\"address\", {}).get(\"postal_code\", \"Unknown\"),\n",
    "                \"street_view_url\": listing.get(\"location\", {}).get(\"street_view_url\", \"Unknown\"),\n",
    "                \"tags\": \", \".join(listing.get(\"tags\", [])) if isinstance(listing.get(\"tags\"), list) else \"Unknown\"  # Extract tags as a comma-separated string\n",
    "            }\n",
    "            data_list.append(sale_record)\n",
    "\n",
    "# Convert extracted data into a DataFrame\n",
    "df = pd.DataFrame(data_list)\n",
    "\n",
    "# Print DataFrame summary to verify structure\n",
    "print(df.info())\n",
    "\n",
    "# Save to CSV for easier analysis\n",
    "df.to_csv(\"expanded_real_estate_listings.csv\", index=False)\n",
    "print(\"CSV file created successfully!\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8159 entries, 0 to 8158\n",
      "Data columns (total 20 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   property_id       8159 non-null   object \n",
      " 1   permalink         8159 non-null   object \n",
      " 2   status            8159 non-null   object \n",
      " 3   year_built        7316 non-null   float64\n",
      " 4   garage            4448 non-null   float64\n",
      " 5   stories           6260 non-null   float64\n",
      " 6   beds              7504 non-null   float64\n",
      " 7   baths             7980 non-null   float64\n",
      " 8   type              8125 non-null   object \n",
      " 9   lot_sqft          6991 non-null   float64\n",
      " 10  sqft              7323 non-null   float64\n",
      " 11  sold_price        6716 non-null   float64\n",
      " 12  sold_date         8159 non-null   object \n",
      " 13  list_price        7721 non-null   float64\n",
      " 14  last_update_date  8125 non-null   object \n",
      " 15  city              8154 non-null   object \n",
      " 16  state             8159 non-null   object \n",
      " 17  postal_code       8159 non-null   object \n",
      " 18  street_view_url   8159 non-null   object \n",
      " 19  tags              8159 non-null   object \n",
      "dtypes: float64(9), object(11)\n",
      "memory usage: 1.2+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       property_id                                          permalink status  \\\n",
      "count         8159                                               8159   8159   \n",
      "unique        1795                                               1795      1   \n",
      "top     6007726455  12312-Birchfalls-Dr_Raleigh_NC_27614_M60077-26455   sold   \n",
      "freq             5                                                  5   8159   \n",
      "mean           NaN                                                NaN    NaN   \n",
      "std            NaN                                                NaN    NaN   \n",
      "min            NaN                                                NaN    NaN   \n",
      "25%            NaN                                                NaN    NaN   \n",
      "50%            NaN                                                NaN    NaN   \n",
      "75%            NaN                                                NaN    NaN   \n",
      "max            NaN                                                NaN    NaN   \n",
      "\n",
      "         year_built       garage      stories         beds        baths  \\\n",
      "count   7316.000000  4448.000000  6260.000000  7504.000000  7980.000000   \n",
      "unique          NaN          NaN          NaN          NaN          NaN   \n",
      "top             NaN          NaN          NaN          NaN          NaN   \n",
      "freq            NaN          NaN          NaN          NaN          NaN   \n",
      "mean    1968.916074     1.926709     1.567732     3.208289     2.131203   \n",
      "std       35.096914     0.878766     0.730969     1.282732     1.175940   \n",
      "min     1828.000000     1.000000     1.000000     0.000000     0.000000   \n",
      "25%     1950.000000     1.000000     1.000000     3.000000     1.000000   \n",
      "50%     1975.000000     2.000000     1.000000     3.000000     2.000000   \n",
      "75%     1997.000000     2.000000     2.000000     4.000000     3.000000   \n",
      "max     2024.000000    11.000000    10.000000    12.000000     9.000000   \n",
      "\n",
      "                 type      lot_sqft          sqft    sold_price   sold_date  \\\n",
      "count            8125  6.991000e+03   7323.000000  6.716000e+03        8159   \n",
      "unique             11           NaN           NaN           NaN         127   \n",
      "top     single_family           NaN           NaN           NaN  2024-01-12   \n",
      "freq             5457           NaN           NaN           NaN        1146   \n",
      "mean              NaN  2.510949e+05   1933.848559  4.126050e+05         NaN   \n",
      "std               NaN  5.823820e+06   1339.039206  6.994308e+05         NaN   \n",
      "min               NaN  0.000000e+00    120.000000  3.080000e+02         NaN   \n",
      "25%               NaN  4.953000e+03   1258.000000  1.910000e+05         NaN   \n",
      "50%               NaN  7.841000e+03   1635.000000  3.140000e+05         NaN   \n",
      "75%               NaN  1.263200e+04   2264.000000  4.700000e+05         NaN   \n",
      "max               NaN  1.676624e+08  32106.000000  2.706500e+07         NaN   \n",
      "\n",
      "          list_price last_update_date        city         state postal_code  \\\n",
      "count   7.721000e+03             8125        8154          8159        8159   \n",
      "unique           NaN             1716         101            45         492   \n",
      "top              NaN       2023-10-20  Harrisburg  Pennsylvania       40601   \n",
      "freq             NaN               95         210           210         210   \n",
      "mean    4.341582e+05              NaN         NaN           NaN         NaN   \n",
      "std     5.514925e+05              NaN         NaN           NaN         NaN   \n",
      "min     1.000000e+00              NaN         NaN           NaN         NaN   \n",
      "25%     2.090000e+05              NaN         NaN           NaN         NaN   \n",
      "50%     3.250000e+05              NaN         NaN           NaN         NaN   \n",
      "75%     4.999000e+05              NaN         NaN           NaN         NaN   \n",
      "max     1.250000e+07              NaN         NaN           NaN         NaN   \n",
      "\n",
      "                                          street_view_url  \n",
      "count                                                8159  \n",
      "unique                                               1792  \n",
      "top     https://maps.googleapis.com/maps/api/streetvie...  \n",
      "freq                                                   10  \n",
      "mean                                                  NaN  \n",
      "std                                                   NaN  \n",
      "min                                                   NaN  \n",
      "25%                                                   NaN  \n",
      "50%                                                   NaN  \n",
      "75%                                                   NaN  \n",
      "max                                                   NaN  \n"
     ]
    }
   ],
   "source": [
    "print(df.describe(include=\"all\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning and Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, ensure that you have all sales in a dataframe.\n",
    "- Take a quick look at your data (i.e. `.info()`, `.describe()`) - what do you see?\n",
    "- Is each cell one value, or do some cells have lists?\n",
    "- What are the data types of each column?\n",
    "- Some sales may not actually include the sale price (target).  These rows should be dropped.\n",
    "- There are a lot of NA/None values.  Should these be dropped or replaced with something?\n",
    "    - You can drop rows or use various methods to fills NA's - use your best judgement for each column \n",
    "    - i.e. for some columns (like Garage), NA probably just means no Garage, so 0\n",
    "- Drop columns that aren't needed\n",
    "    - Don't keep the list price because it will be too close to the sale price. Assume we want to predict the price of houses not yet listed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and concatenate data here\n",
    "# drop or replace values as necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with Tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the fact that with tags, there are a lot of categorical variables.\n",
    "- How many columns would we have if we OHE tags, city and state?\n",
    "- Perhaps we can get rid of tags that have a low frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated new columns after OHE: 1775\n"
     ]
    }
   ],
   "source": [
    "# OHE categorical variables/ tags here\n",
    "# tags will have to be done manually\n",
    "unique_tags = df[\"tags\"].explode().nunique()  # Count unique tags\n",
    "\n",
    "unique_tags = df[\"tags\"].explode().nunique()  # Count unique tags\n",
    "unique_cities = df[\"city\"].nunique()  # Count unique cities\n",
    "unique_states = df[\"state\"].nunique()  # Count unique states\n",
    "\n",
    "total_new_columns = unique_tags + unique_cities + unique_states\n",
    "print(f\"Estimated new columns after OHE: {total_new_columns}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_counts = df[\"tags\"].explode().value_counts()  # Get tag frequencies\n",
    "common_tags = tag_counts[tag_counts > 5].index  # Keep tags appearing more than 5 times\n",
    "\n",
    "df[\"tags\"] = df[\"tags\"].apply(lambda x: [tag for tag in x if tag in common_tags])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"tags\"] = df[\"tags\"].apply(lambda x: \", \".join(x) if isinstance(x, list) else \"Unknown\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exploding Lists Before Encoding\n",
    "df = df.explode(\"tags\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting Lists to Tuples\n",
    "df[\"tags\"] = df[\"tags\"].apply(lambda x: tuple(x) if isinstance(x, list) else (\"Unknown\",))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file created successfully!\n"
     ]
    }
   ],
   "source": [
    "df.to_csv(\"real_estate_listings_with_tags.csv\", index=False)\n",
    "print(\"CSV file created successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with Cities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sales will vary drastically between cities and states.  Is there a way to keep information about which city it is without OHE?\n",
    "- Could we label encode or ordinal encode?  Yes, but this may have undesirable effects, giving nominal data ordinal values.\n",
    "- What we can do is use our training data to encode the mean sale price by city as a feature (a.k.a. Target Encoding)\n",
    "    - We can do this as long as we ONLY use the training data - we're using the available data to give us a 'starting guess' of the price for each city, without needing to encode city explicitly\n",
    "- If you replace cities or states with numerical values (like the mean price), make sure that the data is split so that we don't leak data into the training selection. This is a great time to train test split. Compute on the training data, and join these values to the test data\n",
    "- Note that you *may* have cities in the test set that are not in the training set. You don't want these to be NA, so maybe you can fill them with the overall mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform train test split here\n",
    "# do something with state and city"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Data - STRETCH\n",
    "\n",
    "> This doesn't need to be part of your Minimum Viable Product (MVP). We recommend you write a functional, basic pipeline first, then circle back and join new data if you have time\n",
    "\n",
    "> If you do this, try to write your downstream steps in a way it will still work on a dataframe with different features!\n",
    "\n",
    "- You're not limited to just using the data provided to you. Think/ do some research about other features that might be useful to predict housing prices. \n",
    "- Can you import and join this data? Make sure you do any necessary preprocessing and make sure it is joined correctly.\n",
    "- Example suggestion: could mortgage interest rates in the year of the listing affect the price? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import, join and preprocess new data here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA/ Visualization\n",
    "l of the EDA that you've been learning about?  Now is a perfect time for it!\n",
    "- Look at di\n",
    "Remember alstributions of numerical variables to see the shape of the data and detect outliers.    \n",
    "    - Consider transforming very skewed variables\n",
    "- Scatterplots of a numerical variable and the target go a long way to show correlations.\n",
    "- A heatmap will help detect highly correlated features, and we don't want these.\n",
    "    - You may have too many features to do this, in which case you can simply compute the most correlated feature-pairs and list them\n",
    "- Is there any overlap in any of the features? (redundant information, like number of this or that room...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform EDA here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling and Finishing Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now is a great time to scale the data and save it once it's preprocessed.\n",
    "- You can save it in your data folder, but you may want to make a new `processed/` subfolder to keep it organized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
